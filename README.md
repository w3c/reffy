# Reffy

Reffy is your **W3C spec dependencies exploration companion**. It features a short set of tools to study spec references as well as WebIDL term definitions and references found in W3C specifications.

See [published reports](https://tidoust.github.io/tidoust/reffy-reports/) for human-readable examples of reports generated by Reffy.

## How to use

To launch the crawler and the report study tool, follow these steps:

0. Pre-requisites: [Git](https://git-scm.com/), [Node.js](https://nodejs.org/en/), a [W3C account](https://www.w3.org/accounts/request), an [API key](https://www.w3.org/users/myprofile/apikeys) for the W3C API, and [Pandoc](http://pandoc.org/) if you want to generate an HTML version of the report.
1. Clone the repository: `git clone git@github.com:tidoust/reffy.git`
2. From the root folder of reffy, install required dependencies: `npm install`
3. Create a `config.json` file, initialized with `{ "w3cApiKey": [API key] }`
4. To produce a W3C-centric vision of the Web platform, run `npm run w3c`.
5. To produce a WHATWG-centric vision of the Web platofrm, run `npm run whatwg`.

Under the hoods, these commands run the following steps (and related commands) in turn:
1. **Crawling**: Crawls a list of spec and outputs relevant information in a JSON structure. `node crawl-specs.js ./specs-w3c.json ./reports/w3c/crawl.json`
2. **Analysis**: Analyses the result of the crawling step, and produces a human-readable report in Markdown format. `node study-specs.js ./reports/w3c/crawl.json [perspec|dep]`. By default, the tool generates a report per anomaly, pass `perspec` to create a report per specification, and `dep` to generate a dependencies report. You will probably want to redirect the output to a file, e.g. using `node study-specs.js ./reports/w3c/crawl.json > reports/w3c/index.md`.
3. **Conversion to HTML**: Takes the Markdown analysis per specification and prepares an HTML report with expandable sections. `pandoc reports/w3c/index.md -f markdown -t html5 --section-divs -s --template report-template.html -o reports/w3c/index.html` (where `report.md` is the Markdown report)

Some notes:

* The crawler may take some time
* The crawler uses a local cache for HTTP exchanges. It will create and fill a `cache` subfolder in particular.
* The `./` prefix is needed to point the crawler and study tools at local files for the time being (one of the many things to improve in the code!)

## Reffy's tools

### Specs crawler

**Reffy's crawler** takes an initial list of spec URLs as input and generates a machine-readable report with facts about each spec, including:

1. Generic information such as the title of the spec or the URL of the Editor's Draft. This information is typically extracted from the [W3C API](https://w3c.github.io/w3c-api/).
2. The list of normative/informative references found in the spec.
3. Extended information about WebIDL term definitions and references that the spec contains

### Study tool

**Reffy's report study tool** takes the machine-readable report generated by the crawler, and creates a human-readable Markdown report of *potential* anomalies found in the report, such as:

1. specs that do not seem to reference any other spec normatively;
2. specs that define WebIDL terms but do not normatively reference the WebIDL spec;
3. specs that contain invalid WebIDL terms definitions;
4. specs that use obsolete WebIDL constructs (e.g. `[]` instead of `FrozenArray`);
5. specs that define WebIDL terms that are *also* defined in another spec;
6. specs that use WebIDL terms defined in another spec without referencing that spec normatively;
7. specs that use WebIDL terms for which the crawler could not find any definition in any of the specs it studied;
8. specs that link to another spec but do not include a reference to that other spec;
9. specs that link to another spec inconsistently in the body of the document and in the list of references (e.g. because the body of the document references the Editor's draft while the reference is to the latest published version).

### WebIDL terms explorer

See the related **[WebIDLPedia](https://dontcallmedom.github.io/webidlpedia)** project and its [repo](https://github.com/dontcallmedom/webidlpedia).

### Other tools

Some of the tools used by the crawler may also be used directly.

The **references parser** takes the URL of a spec as input and generates a JSON structure that lists the normative and informative references found in the spec. To run the references parser: `node parse-references.js [url]`

The **WebIDL extractor** takes the URL of a spec as input and outputs the IDL definitions found in the spec as one block of text. To run the extractor: `node extract-webidl.js [url]`

The **WebIDL parser** takes the URL of a spec as input and generates a JSON structure that describes WebIDL term definitions and references that the spec contains. The parser uses [WebIDL2](https://github.com/darobin/webidl2.js/) to parse the WebIDL content found in the spec. To run the WebIDL parser: `node parse-webidl.js [url]`

The **Spec finder** takes a JSON crawl report as input and checks a couple of sites that list Web specifications to detect new specifications that are not yet part of the crawl. To run the spec finder: `node find-spec.js ./results.json`.


For instance:

```bash
node parse-references.js https://w3c.github.io/presentation-api/
node extract-webidl.js https://www.w3.org/TR/webrtc/
node parse-webidl.js https://fetch.spec.whatwg.org/
```

## Technical notes

**Reffy is at an early stage of development and is not stable**. It may crash from time to time and does not report errors/warnings in any meaningful way for the time being.

Reffy should be able to parse most of the W3C/WHATWG specifications that define WebIDL terms (both published versions and Editor's Drafts). The tool may work with other types of specs, but has not been tested with any of them.

### List of specs to crawl

The recommended lists appear in `specs-w3c.json` and `spec-whatwg.json`. Both files reference a common list in `specs-common.json`. These lists were built out of the [JavaScript APIs](http://www.w3.org/TR/#tr_Javascript_APIs) *TR* bucket, semi-manually completed to create a more comprehensive list.

It should be possible to crawl other specs, but note Reffy has not yet been tested with specs that do not define any WebIDL term, and would need to be adjusted to return "interesting" information. Feel free to try out other specs and report any issue!

### Crawling a spec

Given the URL of a spec, the crawler basically goes through the following steps:

1. If the URL looks like `http(s)://www.w3.org/TR/[something]`, the crawler extracts the shortname of the specification, and sends a couple of requests to the W3C API to retrieve the URL of the Editor's Draft, or the URL of the latest published version if the URL of the Editor's Draft could not be found. This new URL replaces the given one.
2. Fetch the URL. Note Reffy uses a network cache on the local filesystem, and sends conditional HTTP requests if the URL is already in that cache
3. Render the response with jsdom, which should create a `Window` object. Note rendering with jsdom may trigger additional fetches (e.g. to retrieve scripts), which also go through the network cache.
4. If the document contains a "head" section that includes a link whose label looks like "single page", go back to step 2 and load the target of that link instead. This makes the crawler load the single page version of multi-page specifications such as HTML5.
5. If needed, wait until the document is properly generated. This is typically needed for specs written with ReSpec that are generated on-the-fly.
6. Run internal tools on the generated document to build the relevant information.

The crawler processes 10 specifications at a time. It may crash from time to time, e.g. because of network errors. Beware: errors are not properly reported yet.

### Config parameters

The crawler reads parameters from the `config.json` file. To be able to interact with the W3C API, that file must contain a `w3cApiKey` entry whose value is a valid W3C API Key.

Optional parameters:

* `avoidNetworkRequests`: set this flag to `true` to tell the crawler to use the cache entry for a URL directly, instead of sending a conditional HTTP request to check whether the entry is still valid. This parameter is typically useful when developing Reffy's code to work offline.
* `resetCache`: set this flag to `true` to tell the crawler to reset the contents of the local cache when it starts.

### Hardcoded rules

Some rules or exceptions to the rule are hardcoded. In particular:

* The URL of some of the Editor's Drafts returned by the W3C API can be invalid, or a document that when loaded redirects to another. The list is hardcoded in the `getSpecFromW3CApi`method in `crawl-specs.js`. The crawler loads the latest published version for these specs.
* Some specs cannot be loaded with jsdom for the time being, typically some specs that use ReSpec's markdown format. This should hopefully be fixed soon. The list is hardcoded in the `getSpecFromW3CApi`method in `crawl-specs.js`.
* Some specs load external scripts that may not run properly in jsdom. Such scripts are ignored. See details in `loadSpecification` function in `util.js`.
* The heuristics used to find the "single page" link are defined in the `loadSpecification` function in `util.js`. They may need to be extended to support other cases.
* For each spec, the crawler reports a list of URLs which may be considered as equivalent for the purpose of referencing. This list typically includes the initial shortname URL for W3C specs, the dated URL of the latest published version of the spec, and the URL of the Editor's Draft. For a couple of specs, it also includes links to previous or alternate "versions" of the spec. For instance, the versions of the HTML5.1 spec include the HTML5 W3C Recommendation and the WHATWG HTML Living Standard. The study tool uses that information when it checks the list of references to find missing ones. Ideally, the W3C API would return up-to-date information such as "supercedes" to clarify the relationship between versions of the same spec. The mapping is hardcoded in `addKnownVersions` in `util.js`.

## Contributing

Authors so far are [François Daoust](https://github.com/tidoust/) and [Dominique Hazaël-Massieux](https://github.com/dontcallmedom/).

Additional ideas, bugs and/or code contributions are most welcome. Create [issues on GitHub](https://github.com/tidoust/issues) as needed!


## Licensing

The code is available under an [MIT license](LICENSE).